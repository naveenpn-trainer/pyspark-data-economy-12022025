# Exploring Spark Core API's - RDD's

## Introduction to RDD's and Partitions

> RDD stands for Resilient Distributed Dataset which are the building blocks of any spark application.

## Partitions

> RDD is a collection of objects that is partitioned and distributed across nodes in a cluster

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe8bCbl-LqfM5ULIsGfZ1lx9dSBJWvYsDB32IoBDHPB2ZUU5MZD9w8jRYbhdJRUzgOnk-KVlWEoZD9UFmzenokIWZGFW10U2VhRhTIjuTtp-4ouWCD426mx8Ltr56LmU5bHZl5UlLqvP9kqefcMz0x-i7aX?key=Dxp7lTxgvspH2ig-I7LuEw)

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfXpUvkdg55DR4i81XE3-vSH-Jz05iDdhWJ_yZN6SaBlQSeL0y3xiio0brqF5Z5019pupssDKTz5pTAaFyX5I6Guh-UyDEqN0ehdFwK61c1iw-g-mQ5gtYGgiii8G189zJrNNowg7080FsAIGfJzaE0Q6fZ?key=Dxp7lTxgvspH2ig-I7LuEw)

## RDD Creation

There are two popular ways to create an RDD

1. Create an RDD from collection

   ```python
   L = list(range(1,101))
   numbers_rdd = sc.parallelize(L)
   type(numbers_rdd)
   ```

2. Create an RDD from external source

   ```
   users_rdd = sc.textFile("c:/u.user")
   type(users_rdd)
   ```

   

**Note**

* All the methods to create an RDD is present inside SparkContext (sc)

## RDD / DataFrame Operations

Once an RDD is created we can perform two types of operations

1. Transformations and 
2. Actions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfe4TfEHNczH4Avf5PmC_0yZA20IELUttWfC6Zi5JxPlXunPZX6oJovslT7T9yQXBO5j5kweGkGdAWzB205x09_1Ho6b5MvcTjd9vBMqvpmYBD82D86Ms3TBAhB5sLyG0dNfUfg8zrqx7iCK1JKfIgym2KL?key=Dxp7lTxgvspH2ig-I7LuEw)



### Transformations

* Transformation create a new RDD/DF from an existing RDD/DataFrame by applying a certain transformation logic.
* Transformations are lazy.
* E.g map(), filterMAp(), groupByKey()
* An RDD depends on zero or more RDD's. THis kind of graph that shows the dependencies of RDD is called **Lineage Graph**

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXec2O43tBwnqeQgRFAg9f4L62OIrBlqSR77AnvHXMJyZT5nUAP2vyIXVUyKOutxIu3BaOyWcCq2ncp63AVjSUjVCoLySG2CT4q8oLOTS5EKMRE5q2Z8er2EYGPaciHkru-3MQ-GQErhd9O7YkIOzqh5p6x9?key=Dxp7lTxgvspH2ig-I7LuEw)

```python
L = list(range(1,101))
numbers_rdd = sc.parallelize(L)
rdd_01 = numbers_rdd.map(lambda e:e*3)
rdd_02 = numbers_rdd.filter(lambda e:e%15==0)
```

### Actions

* Actions are the operations on the RDD/DF to carry out computations and return the result back to the driver program.
* When an action is performed then only the spark job will be triggered.

```
rdd_02.collect()
```



```
rdd_01


rdd_02 = rdd_01
rdd_02.saveAsText()l

rdd_03 = sc.text
```









[('Technician', 1), ('Other', 1), ('Writer', 1), ('Technician', 1), ('Other', 1), ('Technician', 1), ('Administrator', 1), ('Administrator', 1), ('Student', 1), ('Technician', 1)]



("Technician",[1,1,1,1])



rdd.reduceByKey(lambda x,y:x+y)

1+1=2

2+1=3

3,1=4

