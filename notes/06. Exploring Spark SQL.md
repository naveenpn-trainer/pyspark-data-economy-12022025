# Exploring Spark SQL

- [ ] Introduction to Spark SQL
- [ ] Exploring Data Source API
  - [ ] Create DF from different sources
  - [ ] Exploring DataFrameReader API
  - [ ] Different parameters for .csv(), .json()
  - [ ] Exploring DataFrameWriter API
  - [ ] Handling BAD Records

## Introduction

> Spark SQL is one of the module in the spark eco-system to query and analyze structured and semi-structured data.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeJ3v8n_6gl2Xp54NNRQf2EoKUIHdo8iOCZQ2npT_2KY6FMBPTKZytHKOCefXJEZZ9fO0CbQaMAmLmSHS0WfMIILRIJxCY9Ib38KUrEQNXtXbz9uvGSVwGQ06H9Grigau95UX4fhVSKkc9zenkBtxin1WE?key=yGW25KMloT80Lch6YWjT9A)

**Important Points**

* Spark SQL provides a unified API for performing Batch Processing and Stream Processing.

## What is Spark DataFrame

> A DataFrame is a distributed collection of data organized into named columns

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQpKGyhNYPD5Hs2RoUDnr547p53sIWZt46tGe00IKqGhfT4yQ_fv3zIyiBzpPtrgG5w_Qnv43PazAfdSQd5f5MVF7lWcJs_sgz8TUvLEGz_TkIzjN0fAd70JucrNET1rB0rK27uKB9Tr7282R0vWyM_M7m?key=yGW25KMloT80Lch6YWjT9A)

**Important Points**

* Conceptually you can think DF as a in-memory table which is distributed across multiple machines
* After creating a DF; we can perform different operations like .filter, grouping, sorting, aggregation
* DF can be created from variety of different sources.

**Note**

1. All the methods to create DataFrame is present in DataFrameReader API



**Important Points**

* The entry point for the spark functionality is SparkSession

* All the methods to create DataFrame is present inside DataFrameReader

  ```
  dfr = spark.read
  ```

* once a DataFrameReader is created it has several methods to create DataFrame

  * .csv(path, header, inferSchema, quote, sep, schema) -> DataFrame
  * .json(path, multiLine) -> DataFrame
  * .parquet -> DataFrame
  * .jdbc -> DataFrame

* All the builtin functions is present

  ```
  from pyspark.sql.functions import col, split, array_contains, lit, when, max, 
  ```

* DataFrame methods (Transformations and Actions)

  * .show(n=30, truncate=False)
  * .select
  * .filter, .where
  * .printSchema
  * .withColumn
  * .withColumnRenamed
  * .drop
  * groupBy
  * .count
  * .columns
  * .selectExpr



**Array Types**

```
id|name|exp|gen|dob|company|desig|doj|col_skills|col_actual_expected_salary
```

from pyspark.sql.functions import split(col, pattern)

**Prerequisite**

1. Create array columns

```python
from pyspark.sql.functions import split
employee_df = spark.read.csv()

# Create Array columns
result_df = employee_df.withCOlumn("skills", split(col("col_skills"),",")).withCOlumn("actual_expected_salary", split(col("col_actual_expected_salary"),",")).drop("col_skills","col_actual_expected_salary")


```



**Problem Statements**

1. 