# Getting Started with Apache Spark

> Apache Spark is an **In-memory** cluster computing framework designed to handle a wide range of big data

**Applications of Apache Spark**

* Data Integration and ETL
* Batch Computation
* Machine Learning Analytics
* Real-time stream processing



**Important Points**

* Apache Spark is natively written using Scala programming

## What is PySpark

> PySpark is the Python API for APache Spark.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc_M5I5eoMQok4DtKjqPSUgQ8sBeqpMeQDxnB9ZLmax1Ue8yCXqurT8wFpbA1PpmSKuwE97o49UeuQuFChJft4J5K8oyJjZJmKcO21i5ZGuFDRZ742rZ7Niz1lwtJcLQDh9dowbAAbGJICxmG5tWImfzWEz?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcPMTFn-UpeNYVbf3HOdbHAoe3p0aGmsngwpgdOcUfBoE7qiV4Xe3adwNYs0IXd-Qx1PZm-icX5qDcT7KBZ6yIoXSuLN7Ts2dh5gdMEtdv11eClmuMHIX9UaYQiVhLf1Nc6p909lTi0fMLy-VJ4VGBUUSzd?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Interactive Shell

It is a command line interface provided by Apache Spark.

1. Spark Shell (Scala)

   ```
   $> spark-shell
   ```

   

2. PySpark (Python)

   ```
   $> pyspark
   ```

   

## Spark APIs for Data Processing

1. Low Level API (RDD's) (Deprecated) - Unstructured APIs 
2. High Level API : (DataFrame, Dataset API) - Structured API's

## Building a Spark Application

1. Load the data
2. Process the data
3. Write the result to different destination system.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZvR53mazCiqCv6R1N08JK4V0btNCF9qKGAJGeggVuv-MS4mxVa1l99RO31oZig-5qFgJ1DJqAuZgVmw7xg6JQTOgjKqKTS3MCoo1IbZJL7SRDYeymdGoQVLN-sChM4bpfFsXLKeOalJu83rhVr3CVuB8?key=_he-T4Jq934AhrSZa-Be-g)

